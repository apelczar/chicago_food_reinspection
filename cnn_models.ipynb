{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "from torchtext.vocab import GloVe\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "\n",
    "if USE_CUDA:\n",
    "    DEVICE = torch.device('cuda')\n",
    "    print(\"Using cuda.\")\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "    print(\"Using cpu.\")\n",
    "    \n",
    "random.seed(30255)\n",
    "np.random.seed(30255)\n",
    "torch.manual_seed(30255)\n",
    "if USE_CUDA:\n",
    "    torch.cuda.manual_seed(30255)\n",
    "    \n",
    "COLAB = False\n",
    "#COLAB = True\n",
    "if COLAB:\n",
    "    from google.colab import drive \n",
    "    drive.mount('/content/gdrive')\n",
    "    PATH = \"gdrive/My Drive/advanced_ml/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /Users/michaelfeldman/opt/anaconda3/include/python3.7m/UNKNOWN\n",
      "sysconfig: /Users/michaelfeldman/opt/anaconda3/include/python3.7m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n",
      "Requirement already satisfied: pytorch-ignite in /Users/michaelfeldman/opt/anaconda3/lib/python3.7/site-packages (0.4.4)\n",
      "Requirement already satisfied: torch<2,>=1.3 in /Users/michaelfeldman/opt/anaconda3/lib/python3.7/site-packages (from pytorch-ignite) (1.8.0)\n",
      "Requirement already satisfied: numpy in /Users/michaelfeldman/opt/anaconda3/lib/python3.7/site-packages (from torch<2,>=1.3->pytorch-ignite) (1.19.2)\n",
      "Requirement already satisfied: typing-extensions in /Users/michaelfeldman/opt/anaconda3/lib/python3.7/site-packages (from torch<2,>=1.3->pytorch-ignite) (3.7.4.3)\n",
      "\u001b[33mWARNING: Value for scheme.headers does not match. Please report this to <https://github.com/pypa/pip/issues/9617>\n",
      "distutils: /Users/michaelfeldman/opt/anaconda3/include/python3.7m/UNKNOWN\n",
      "sysconfig: /Users/michaelfeldman/opt/anaconda3/include/python3.7m\u001b[0m\n",
      "\u001b[33mWARNING: Additional context:\n",
      "user = False\n",
      "home = None\n",
      "root = None\n",
      "prefix = None\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch-ignite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if COLAB:\n",
    "    df = pd.read_pickle(PATH + 'initial_clean.pkl')\n",
    "else:\n",
    "    df = pd.read_pickle('initial_clean.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df currently has 37207 rows\n",
      "df now has 36553 rows\n"
     ]
    }
   ],
   "source": [
    "print(f'df currently has {df.shape[0]} rows')\n",
    "\n",
    "# Remove rows with no comments\n",
    "df = df[df['violations_orig'].notna()]\n",
    "print(f'df now has {df.shape[0]} rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 'Pass w/ Conditions' to 'Pass' and change target to 0/1 numeric\n",
    "results_dict = {'Pass': 0, 'Fail': 1}\n",
    "df['results_re'] = df['results_re'].str.replace('Pass w/ Conditions', 'Pass')\n",
    "df['results_re'] = df['results_re'].apply(lambda x: results_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data into feature & target, and then into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort dataframe by inspection date\n",
    "# Training set will include initial 80% of inspections\n",
    "sorted_df = df.sort_values(by=['date_orig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = ['results_re']\n",
    "target = sorted_df[target_col]\n",
    "\n",
    "cols_to_exclude = ['name', 'id_orig', 'id_re', 'license']\n",
    "feature_cols = [col for col in sorted_df.columns if (col not in cols_to_exclude and col not in target_col)]\n",
    "features = sorted_df[feature_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't shuffle data before splitting\n",
    "train_feat, test_feat, train_targ, test_targ = train_test_split(features, target, test_size=0.2,\n",
    "                                                                shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.76% reinspections fail in training set\n"
     ]
    }
   ],
   "source": [
    "pct_fail = len(train_targ[train_targ['results_re'] == 1]) / len(train_targ)\n",
    "print(f'{pct_fail*100:.2f}% reinspections fail in training set')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split out the text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_col = ['violations_orig']\n",
    "train_feat_txt = train_feat[text_col].astype(str)\n",
    "test_feat_txt = test_feat[text_col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>violations_orig</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>220538</th>\n",
       "      <td>18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>494292</th>\n",
       "      <td>18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45775</th>\n",
       "      <td>18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>488807</th>\n",
       "      <td>35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>455503</th>\n",
       "      <td>21. * CERTIFIED FOOD MANAGER ON SITE WHEN POTE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          violations_orig\n",
       "220538  18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...\n",
       "494292  18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...\n",
       "45775   18. NO EVIDENCE OF RODENT OR INSECT OUTER OPEN...\n",
       "488807  35. WALLS, CEILINGS, ATTACHED EQUIPMENT CONSTR...\n",
       "455503  21. * CERTIFIED FOOD MANAGER ON SITE WHEN POTE..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feat_txt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split train set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, val_x, train_y, val_y = train_test_split(train_feat_txt, train_targ,\n",
    "                                                    test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample on target vector\n",
    "train_targ_reset = train_y.reset_index().drop('index', axis=1)\n",
    "train_targ_fail = train_targ_reset[train_targ_reset['results_re'] == 1]\n",
    "size_diff = train_targ_reset.shape[0] - train_targ_fail.shape[0]\n",
    "train_resample = resample(train_targ_fail, n_samples = size_diff, replace=True)\n",
    "train_targ_all = pd.concat([train_targ_reset, train_resample])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then resample features\n",
    "train_x_reset = train_x.reset_index().drop('index', axis=1)\n",
    "train_x_resample = train_x_reset.iloc[train_targ_all.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine text and label into 1 df\n",
    "df_train = pd.concat([train_x_resample, train_targ_all], axis=1)\n",
    "df_valid = pd.concat([val_x, val_y], axis=1)\n",
    "df_test = pd.concat([test_feat_txt, test_targ], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: Data prep informed by [notebook](https://www.kaggle.com/swarnabha/pytorch-text-classification-torchtext-lstm) from Kaggle user swarnabha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.legacy.data import (Field, LabelField, TabularDataset,\n",
    "                                   BucketIterator, Example, Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Field objects to process the text data\n",
    "# these will include info for how to convert the text to tensors\n",
    "TEXT = Field(tokenize='basic_english', lower=True, batch_first=True)\n",
    "LABEL = LabelField(dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source:\n",
    "# https://www.kaggle.com/swarnabha/pytorch-text-classification-torchtext-lstm\n",
    "# Allows us to convert a pandas DataFrame to a Dataset object\n",
    "class DataFrameDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, fields, **kwargs):\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.results_re\n",
    "            text = row.violations_orig\n",
    "            examples.append(Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields)\n",
    "\n",
    "    @staticmethod\n",
    "    def sort_key(ex):\n",
    "        return len(ex.violations_orig)\n",
    "\n",
    "    @classmethod\n",
    "    def splits(cls, fields, train_df, val_df=None, test_df=None, **kwargs):\n",
    "        train_data, val_data, test_data = (None, None, None)\n",
    "        data_field = fields\n",
    "\n",
    "        if train_df is not None:\n",
    "            train_data = cls(train_df.copy(), data_field, **kwargs)\n",
    "        if val_df is not None:\n",
    "            val_data = cls(val_df.copy(), data_field, **kwargs)\n",
    "        if test_df is not None:\n",
    "            test_data = cls(test_df.copy(), data_field, True, **kwargs)\n",
    "\n",
    "        return tuple(d for d in (train_data, val_data, test_data) if d is not None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Dataset objects\n",
    "fields = [('violations_orig', TEXT), ('results_re', LABEL)]\n",
    "train_ds, val_ds, test_ds = DataFrameDataset.splits(fields, train_df=df_train,\n",
    "                                                    val_df=df_valid, test_df=df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FREQ = 500\n",
    "\n",
    "# Build vocab using training set\n",
    "TEXT.build_vocab(train_ds, vectors='glove.6B.300d', min_freq=MIN_FREQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(None, {1: 0, 0: 1})"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build vocab for labels\n",
    "LABEL.build_vocab(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure labels aren't inadvertently swapped\n",
    "from collections import defaultdict\n",
    "d = defaultdict(None)\n",
    "d[1] = 1\n",
    "d[0] = 0\n",
    "LABEL.vocab.stoi = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build iterators\n",
    "BATCH_SIZE=64\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
    "    (train_ds, val_ds, test_ds),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort_key=lambda x: len(x.violations_orig),\n",
    "    device = DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Model Building, Training, & Evaluation\n",
    "\n",
    "_Notes:_\n",
    "- _Model architecture and training process informed by [tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb) from Ben Trevett_\n",
    "- _Cells do not have output, as all model training was completed on Colab_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        # This gives us n_filters number of filters for every filter size. So we get\n",
    "        # n_filters * len(filter_sizes) total filters\n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                 \n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        conved = [F.leaky_relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparams to tune\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 10\n",
    "FILTER_SIZES = [2, 3, 4, 5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "LR = 0.00005\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES,\n",
    "            OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "\n",
    "# Load pretrained embeddings\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "\n",
    "# Zero out the initial weights of the unknown and padding tokens\n",
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "criterion = criterion.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy for a given batch\n",
    "    \n",
    "    Inputs:\n",
    "        preds: tensor with model predictions\n",
    "        y: tensor with ground truth values\n",
    "        \n",
    "    Returns:\n",
    "        Accuracy of the batch (float)\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()\n",
    "    return correct.sum() / len(correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_interval = 100\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    '''\n",
    "    Train the model for one epoch\n",
    "    \n",
    "    Inputs:\n",
    "        model: a model object\n",
    "        iterator: an iterator object\n",
    "        optimizer: the optimizer for the model\n",
    "        criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy over the epoch (both floats)\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i, batch in enumerate(iterator):\n",
    "        optimizer.zero_grad()\n",
    "        predictions = model(batch.violations_orig).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.results_re)\n",
    "        acc = get_accuracy(predictions, batch.results_re)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % log_interval == 0 and i > 0:\n",
    "            print(f'At iteration {i} the training accuracy is {acc:.3f}.')\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    '''\n",
    "    Evaluate the model over an epoch\n",
    "    \n",
    "    Inputs:\n",
    "        model: a model object\n",
    "        iterator: an iterator object\n",
    "        criterion: loss function\n",
    "    \n",
    "    Returns:\n",
    "        Average loss and accuracy over the epoch (both floats)\n",
    "    '''\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            predictions = model(batch.violations_orig).squeeze(1)\n",
    "            loss = criterion(predictions, batch.results_re)\n",
    "            \n",
    "            acc = get_accuracy(predictions, batch.results_re)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    '''\n",
    "    Calculate the amount of time that has elapsed over an epoch\n",
    "    \n",
    "    Inputs:\n",
    "        start_time: Float, time the epoch started\n",
    "        end_time: Float, time the epoch ended\n",
    "        \n",
    "    Returns:\n",
    "        Tuple, number of minutes and seconds that elapsed\n",
    "        over the epoch\n",
    "    '''\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "\n",
    "N_EPOCHS = 8\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "# reset/clear out model just in case\n",
    "for layer in model.children():\n",
    "    if hasattr(layer, 'reset_parameters'):\n",
    "        layer.reset_parameters()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator,\n",
    "                                  optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator,\n",
    "                                     criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    # copy model if it's the best one so far\n",
    "    if valid_loss < best_valid_loss:\n",
    "        print('Best model yet is found! Saving it!')\n",
    "        best_valid_loss = valid_loss\n",
    "        model_copy = type(model)(INPUT_DIM, EMBEDDING_DIM, N_FILTERS,\n",
    "                                 FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)\n",
    "        model_copy.load_state_dict(model.state_dict())\n",
    "        best_model = model_copy\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ignite.metrics import Precision\n",
    "from ignite.metrics import Recall\n",
    "from ignite.metrics import Accuracy\n",
    "\n",
    "def get_test_metrics(iterator):\n",
    "    '''\n",
    "    Print out metrics for best model from predictions\n",
    "    on a data iterator\n",
    "    '''\n",
    "    best_model.to(DEVICE)\n",
    "    best_model.eval()\n",
    "    precision = Precision()\n",
    "    recall = Recall()\n",
    "    accuracy = Accuracy()\n",
    "    f1 = (precision * recall * 2 / (precision + recall)).mean()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            preds = best_model(batch.violations_orig).squeeze(1)\n",
    "            rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "            precision.update((rounded_preds, batch.results_re))\n",
    "            recall.update((rounded_preds, batch.results_re))\n",
    "            accuracy.update((rounded_preds, batch.results_re))\n",
    "            f1.update((rounded_preds, batch.results_re))\n",
    "\n",
    "    print('Precision: ', precision.compute())\n",
    "    print('Recall: ', recall.compute())\n",
    "    print('F1: ', f1.compute())\n",
    "    print('Accuracy: ', accuracy.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics for best model on validation set\n",
    "get_test_metrics(valid_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics for best model on test set\n",
    "get_test_metrics(test_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
